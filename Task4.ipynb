# Step 1: Import necessary libraries
import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import confusion_matrix
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
# Step 2: Download, extract, and preprocess images

# 2.1 Download dataset zip
!wget -O asl_digits.zip "https://github.com/ardamavi/Sign-Language-Digits-Dataset/archive/refs/heads/master.zip"

# 2.2 Extract dataset
import zipfile
with zipfile.ZipFile("asl_digits.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/")

# 2.3 Set dataset path
dataset_path = "/content/Sign-Language-Digits-Dataset-master/Dataset"  # correct path
print("Dataset path:", dataset_path)

# 2.4 List all gesture classes
import os
gesture_classes = os.listdir(dataset_path)
print("Detected gesture classes:", gesture_classes)

# 2.5 Load and preprocess images
import cv2
import numpy as np
from tqdm import tqdm

IMG_SIZE = 64  # Resize all images to 64x64
data = []
labels = []

print("ðŸ“¥ Loading and preprocessing images...")
for gesture in gesture_classes:
    class_path = os.path.join(dataset_path, gesture)
    for img_name in tqdm(os.listdir(class_path), desc=f"Loading {gesture}"):
        img_path = os.path.join(class_path, img_name)
        try:
            # Read image in grayscale
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            # Resize
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            data.append(img)
            labels.append(gesture)
        except:
            continue

# Convert to numpy arrays and normalize
data = np.array(data, dtype="float32") / 255.0
data = np.expand_dims(data, axis=-1)  # Add channel dimension
labels = np.array(labels)

print("Data shape:", data.shape)
print("Labels shape:", labels.shape)
# Step 3: Encode labels and split dataset
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split

# 3.1 One-hot encode labels
lb = LabelBinarizer()
labels_encoded = lb.fit_transform(labels)
print("Classes detected:", lb.classes_)
print("Labels encoded shape:", labels_encoded.shape)

# 3.2 Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data, labels_encoded,
    test_size=0.2,
    random_state=42,
    stratify=labels_encoded
)

print("Train set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)
# Step 4: Define CNN architecture
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

model = Sequential()

# Convolutional layers
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 1)))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))

# Flatten and fully connected layers
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(gesture_classes), activation='softmax'))  # Output layer

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Show model summary
model.summary()
# Step 5: Train the CNN model
EPOCHS = 15
BATCH_SIZE = 32

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)
# Step 6: Plot training & validation accuracy and loss
import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))

# Accuracy plot
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='green')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss', color='green')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()
# Step 7: Evaluate model
from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np

# 7.1 Evaluate on test set
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"ðŸŽ¯ Test Accuracy: {test_acc*100:.2f}%")

# 7.2 Predict labels for test set
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

# 7.3 Confusion matrix
cm = confusion_matrix(y_true, y_pred)

# 7.4 Plot confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=gesture_classes, yticklabels=gesture_classes)
plt.title("Confusion Matrix - Hand Gesture Recognition", fontsize=14)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()
# Step 8: Display 9 random sample predictions
plt.figure(figsize=(10,8))

for i in range(9):
    idx = np.random.randint(0, len(X_test))
    img = X_test[idx].reshape(64, 64)
    true_label = gesture_classes[y_true[idx]]
    pred_label = gesture_classes[y_pred[idx]]

    plt.subplot(3,3,i+1)
    plt.imshow(img, cmap='gray')
    color = 'green' if true_label == pred_label else 'red'
    plt.title(f"T:{true_label}\nP:{pred_label}", color=color)
    plt.axis('off')

plt.suptitle("Sample Predictions (Green=Correct, Red=Wrong)", fontsize=16)
plt.show()
# Step 9.1: Save the trained model
model.save("hand_gesture_cnn_model.h5")
print("âœ… Model saved as hand_gesture_cnn_model.h5")

# Step 9.2: Save the label encoder
import pickle
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(lb, f)
print("âœ… Label encoder saved as label_encoder.pkl")

# Step 9.3: Load the model and label encoder (example)
from tensorflow.keras.models import load_model

loaded_model = load_model("hand_gesture_cnn_model.h5")
with open("label_encoder.pkl", "rb") as f:
    loaded_lb = pickle.load(f)
print("âœ… Model and label encoder loaded successfully")

# Step 9.4: Test loaded model on a random sample
sample_idx = np.random.randint(0, len(X_test))
sample_img = X_test[sample_idx].reshape(1, 64, 64, 1)
pred_label_idx = np.argmax(loaded_model.predict(sample_img), axis=1)[0]

print(f"Sample True Label: {gesture_classes[y_true[sample_idx]]}")
print(f"Sample Predicted Label: {gesture_classes[pred_label_idx]}")
# Step 10: Real-Time Gesture Prediction (Image Upload in Colab)
from google.colab import files
import cv2
import numpy as np
from tensorflow.keras.models import load_model
import pickle
from matplotlib import pyplot as plt

# Load model and label encoder
model = load_model("hand_gesture_cnn_model.h5")
with open("label_encoder.pkl","rb") as f:
    lb = pickle.load(f)

# Upload an image
uploaded = files.upload()
for file_name in uploaded.keys():
    # Read the uploaded image
    img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)
    img_resized = cv2.resize(img, (64,64))
    img_norm = img_resized.astype('float32') / 255.0
    img_input = np.expand_dims(img_norm, axis=(0,-1))  # Shape: (1,64,64,1)

    # Predict gesture
    pred_prob = model.predict(img_input)
    pred_class = lb.classes_[np.argmax(pred_prob)]

    # Display
    plt.imshow(img, cmap='gray')
    plt.title(f"Predicted Gesture: {pred_class}")
    plt.axis('off')
    plt.show()
